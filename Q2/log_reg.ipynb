{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e784ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import necessary packages\n",
    "\"\"\"\n",
    "\n",
    "# necessary imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mnist_reader import *\n",
    "\n",
    "\"\"\"\n",
    "Complete checks to ensure status of TensorFlow\n",
    "\"\"\"\n",
    "\n",
    "# ensure eager execution\n",
    "print(\"Eager execution:\", tf.executing_eagerly())\n",
    "\n",
    "# check if GPU is available on system\n",
    "device_name = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if(device_name):\n",
    "    print(f\"GPU Available: {device_name}\")\n",
    "else:\n",
    "    print(\"CPU Only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886e5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:        (54000, 784), (54000,)\n",
      "Validation Set:   (6000, 784) , (6000,)\n",
      "Test Set :        (10000, 784), (10000,)\n"
     ]
    }
   ],
   "source": [
    "# define parameters for the model\n",
    "learning_rate = None\n",
    "batch_size = 32\n",
    "n_epochs = None\n",
    "n_train = None\n",
    "n_test = None\n",
    "valid_fract = 0.1\n",
    "\n",
    "# Step 1: Read in data\n",
    "\n",
    "# read in the data\n",
    "fmnist_folder = \".\"\n",
    "X_train_all, y_train_all = load_mnist(fmnist_folder, kind='train')\n",
    "X_test, y_test = load_mnist(fmnist_folder, kind='t10k')\n",
    "\n",
    "# compute fraction of training data being used for validation\n",
    "num_val = int(len(X_train_all) * valid_fract)\n",
    "\n",
    "# shuffle to ensure random validation set\n",
    "indices = np.arange(len(X_train_all))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_train_all = X_train_all[indices]\n",
    "y_train_all = y_train_all[indices]\n",
    "\n",
    "# split training data into training and validation subsets\n",
    "X_val = X_train_all[:num_val]\n",
    "y_val = y_train_all[:num_val]\n",
    "X_train = X_train_all[num_val:]\n",
    "y_train = y_train_all[num_val:]\n",
    "\n",
    "# display train, validation, and test split for check\n",
    "print(f\"Train Set:        {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation Set:   {X_val.shape} , {y_val.shape}\")\n",
    "print(f\"Test Set :        {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2828c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: create datasets and batch them\n",
    "\n",
    "# create training dataset and then batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_data = train_data.shuffle(buffer_size = X_train.shape[0]).batch(batch_size)\n",
    "\n",
    "# create test dataset and then batch it\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: (784, 10)\n",
      "Bias shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: create weights and bias\n",
    "\n",
    "# define number of features and classes\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = 10 \n",
    "\n",
    "# initialize w to random variables with mean of 0, stddev of 0.01\n",
    "w = tf.Variable(tf.random.normal([n_features, n_classes], mean = 0.0, stddev = 0.01))\n",
    "\n",
    "# initialize b with zeros (one for each class)\n",
    "b = tf.Variable(tf.zeros([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: build model\n",
    "\n",
    "# define the logistic regression model\n",
    "def logistic_regression(X, w, b):\n",
    "    return tf.matmul(X, w) + b\n",
    "\n",
    "# start the training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in train_data:\n",
    "        logits = logistic_regression(X_batch, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "loss = None\n",
    "#############################\n",
    "########## TO DO ############\n",
    "#############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS599",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
